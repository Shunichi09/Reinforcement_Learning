# Outline
1. 強化学習って結局？
2. 用語再び
3. 今回は理論編なので．．．

# 強化学習って結局？
前回、N本腕バンディット問題(良い腕を選ぶってだけですが)を解きました
結局あれは、何をしているのでしょうか？
改めて復習も兼ねて以下の図をもう一度見てみましょう

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/492e58b7-b282-98be-43db-805ccb30c9b5.png)

これはよくある強化学習の図です
エージェントが行動し，環境がそれに応じて，状態を変化させて，報酬をエージェントに返すわけです

ここでいう今後もらえる報酬の和を最大にしよう！というのが強化学習のモチベーションでした

つまりその和が**価値**になるわけです
価値(これから貰えるだろーなっていう報酬の和)が最大になるような行動を取っているわけです

ということは
以下のことが強化学習にとって最も重要になります

- **ある状態の価値を知ること**
（方策ベースの話もありますがいまはこれだけで！）

これを知れれば、あとは価値の高い状態に移り変わるように行動すればきっとうまくいきます

# ここから数学のお話
強化学習では基本的に，確率（期待値）で物事が進んでいきます
なので，残念ながらここからは今後のために出てくる言葉の意味の説明と定式化をしていかなければなりません
**しかし！しかしではないかもしれませんが笑,そんなに難しくありません**
ので，ゆっくりとみていただけたらと思います．

## 方策
これはある状態において，エージェント（ロボット）が行える行動の中から何を選ぶかの確率を表したものです

<img src=https://latex.codecogs.com/gif.latex?\pi_t(s,a)>

この式はある時刻tにおいてロボットの状態がsの時，aを選ぶ確率となります．

図で書くと下のイメージ

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/5a71216d-e433-c587-2485-22641156d862.png)

迷路でのイメージですね！
ちなみに添え字のtについてはガン無視してますが，これは時刻が関係する場合に考慮します（時間によって報酬が変わるなど）今は複雑になるので考慮してません

## 報酬
ある行動をとった結果もらえるご褒美的なものです．

<img src=https://latex.codecogs.com/gif.latex?r_t>

で表されることが多く，これで，時刻$t$にもらった報酬になります

例えば，ある迷路に挑戦して，ゴールをもらった場合，+1
ゴールしない行動を選択しつづけた場合は-1とするみたいな例があります

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/42a36360-4727-699f-725b-93a8d6da9d45.png)

水色⇒は省略してます

## 収益
ある時刻からみた，1つのゲームなど時刻の終わりまでプレーした際の報酬の合計になります．
単純に上の図の例なら，ゴールしたら1つのゲームが終わるので，ゴールするまでの報酬をすべて足し算すればよいわけですね

<img src=https://latex.codecogs.com/gif.latex?R_t=r_{t&plus;1}&plus;r_{t&plus;2}&plus;r_{t&plus;3}&plus;r_{t&plus;4}&plus;...r_{t_infty}>

これで，ある時刻tからみた収益になります（そのあとの時刻でもらえる報酬をすべて足せばよいわけです）
これを最大化できる方策をゲットすればなんとなく良いような気がします

## 割引率
上の収益の式だと，迷路ゲームのように終わりがなく，永久に続くものでは（振り子の制御など），収益が発散し，方策をうまく選択できなくなります．
なので，過去の報酬を割引することで，報酬や収益を収束させようという魂胆です

<img src=https://latex.codecogs.com/gif.latex?R_t&space;=&space;r_{t&plus;1}&plus;&space;\gamma&space;r_{t&plus;2}&plus;&space;\gamma^2&space;r_{t&plus;3}&plus;&space;\gamma^3&space;r_{t&plus;4}&plus;...r_{t_infty}=\sum_{k=0}^{\infty}&space;\gamma&space;r_{t&plus;k&plus;1}>

もちろん割引率のγです．収束しなくなるので
ちなみにγ=0とすれば，即時報酬（行動に対してもらえる報酬のみ意識する）ことになります

## マルコフ性
でてきました
このマルコフさんです．音声認識などの時系列などを考える問題にもつきまとってきます
詳細は僕自身もあまり理解していないので，詳しいことを知りたい場合は，Google先生等にお願いします．

ただここで大事なのは
t+1における環境の応答はtにおける状態と行動にのみ依存すること
つまり
すべて，今時刻の行動にのみ依存するということです．

過去の行動は関係ありませんというと少し誤解があるかもしれませんが，

正確に言うなら，強化学習ではt+1をtのみの情報で表現できるので問題を解くことができるんですね
ちなみにマルコフ性に従うタスクをマルコフ決定過程といい，状態が空間が離散の場合，有限マルコフ決定過程といいます

## 状態価値関数
さて，本回で最も大事なものが出てきました
この価値関数はある状態での価値を表した関数です．
つまり，その状態になるとどれくらい収益を今後もらえる可能性があるのかを定義したものということです．

実際の式を見てみましょう

<img src=https://latex.codecogs.com/gif.latex?V^\pi(s)&space;=&space;E_\pi&space;\bigl[R_t&space;|&space;s_t&space;=&space;s]&space;=&space;E_\pi&space;\bigl[\sum_{k=0}^{\infty}&space;\gamma&space;r_{t&plus;k&plus;1}|s_t&space;=&space;s]>


![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/fa6c1c98-40eb-2938-55a5-ac45d722b53b.png)

**つまり，状態sにいて，方策πをとったときのどれだけ収益をもらえるか考えているわけです．**
<img src=https://latex.codecogs.com/gif.latex?E_\pi>は期待値を表します

## 行動価値関数
さきほどはある状態sにいて，方策πに従うとしたときのでした
なので次は
さきほどはある状態sにいて，行動aをとり，そのあとは，方策πに従った場合の行動価値関数を定式化します

<img src=https://latex.codecogs.com/gif.latex?Q^\pi(s)&space;=&space;E_\pi&space;\bigl[R_t&space;|&space;s_t&space;=&space;s,&space;a_t&space;=&space;a]&space;=&space;E_\pi&space;\bigl[\sum_{k=0}^{\infty}&space;\gamma&space;r_{t&plus;k&plus;1}|s_t&space;=&space;s,&space;a_t&space;=&space;a]>

a_t = aという行動の条件が加わっているだけですね
つまり，状態価値関数の中で，ある時刻tの行動が指定されているイメージになります

上記2つをまとめるとこういう感じ
まとめているというか書いてるだけですが．．．

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/5a956af1-6e1d-8b58-5949-17ef3189892e.png)


## 状態価値関数（再考）
さて，先ほどの式だとなんかよくわからないので再考しましょう
そもそも期待値ってどうやって処理するんですかという話なわけです

先ほどの式を見つめると
後半部分を以下のように考えることができます．

- ある状態でもらえる報酬の和（期待収益）
⇒
- ある状態で行動aをとってすぐにもらえる即時報酬＋そのあとでもらえる期待収益
⇒
- ある状態で行動aをとってすぐにもらえる即時報酬＋そのあと移り変わった状態でもらえる期待収益


つまり，今の状態sの価値関数は移り変わった状態（s'）の価値関数で表現できることが分かります．

よって式はこんな風に変換できます

<img src=https://latex.codecogs.com/gif.latex?V^\pi(s)&space;=&space;\sum_{a}&space;\pi(s,&space;a)&space;\sum_{s'}&space;\rho_{ss'}^a[r_{ss'}^a&space;&plus;&space;\gamma&space;V^\pi(s')]>

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/7079f892-da2e-2de9-53ac-80d8e54ff347.png)

## 最適行動価値関数・最適状態価値関数
さきほどの式が理解できれば今回はおっけーなのかとおもいきや，最適行動価値関数というものも登場するので考えてみましょう．
最適状態価値関数は

<img src=https://latex.codecogs.com/gif.latex?V^*(s)&space;=&space;\max_{\pi}&space;V^\pi(s)>

ある状態で，状態価値関数を最大化する最適方策πを表しています

次に
最適行動価値関数は

<img src=https://latex.codecogs.com/gif.latex?Q^*(s,&space;a)&space;=&space;\max_{\pi}&space;Q^\pi(s,&space;a)>

これは，ある状態で行動aをとりそののちは行動価値関数を最大化する最適方策に従うことを表しています

はい
ちょっと複雑になってきましたが，要は，最適行動価値関数と最適状態価値関数を最大化するような最適方策πでの話ですね．

これもさっきと同じように変換してみると

<img src=https://latex.codecogs.com/gif.latex?V^*(s)&space;=&space;\max_{a}&space;\sum_{s'}&space;\rho_{ss'}^a[r_{ss'}^a&space;&plus;&space;\gamma&space;V^*(s')]>

よって，ある状態$s$での最適状態価値関数は，移り変わった状態の最適価値関数を用いて表現できることがわかります
これは，最適＝最適＋最適だという，ベルマンさんの最適性の原理に従っているやつです．

さて同じく最適行動価値関数も以下のように算出できます

<img src=https://latex.codecogs.com/gif.latex?Q^*(s,&space;a)&space;=&space;\sum_{s'}&space;\rho_{ss'}^a[r_{ss'}^a&space;&plus;&space;\gamma&space;\max_a'&space;Q^*(s',&space;a')]>

さて，これが分かったからなにがいいんだよというわけなのですが

- 最適状態価値関数V(s)が分かっているということは，greedyな方策が最適方策になります
- 最適行動価値関数Q(s, a)が分かっているということは，単純に最適行動価値関数を最大化する行動aを行えばよいことになります

上についてですが，最適状態価値関数が分かっているのだから，次の状態の価値も分かります
なので，次の状態とその他もろもろを計算すればよくて，その計算結果がもっとも大きいものを選べばよいのです
なのでgreedy方策です

一方下についてですが，
もはやある行動aをとった場合の値が分かっているので，単純にどの行動をとればいいかはすぐにわかりますね
さらに言いますと
行動a'までもすぐにわかるので，もはやすべての行動を決定することができます
なんとなくゴールからさかのぼればよさそうなにおいがぷんぷんします!

