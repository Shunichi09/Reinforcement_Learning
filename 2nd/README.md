# 2nd
このフォルダでは、バンディット問題について説明します。

# Outline
1. n本腕バンディット問題
    2. 問題の概要と説明
    3. プログラム実行法（Usage）
    4. 考察

# n本腕バンディット問題とは
聞こえだけだけだとなにやらとっても難しそうですが
やる問題としてはいたって簡単です
腕が何本かついているスロットを用意します
腕を引くとスロットが回って，いくらかの利益（**報酬**）がもらえます
どの腕を引けばよいでしょうか？それを学習してください
という問題です

以下の図を参考に！
![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/83f4290f-dfb1-bc40-1b50-0ababc7425c8.png)

## 問題の定式化と強化学習への落とし込み
問題の定式化を行います　ここから数学の話がでてきますが，まだ簡単ですので大丈夫です（第四回ぐらいから急に来ます）
この問題で言う
### エージェント
スロットマシンを引くロボット？というか引く人

### 行動
どの腕を引くか
（行動a）とします

### 環境
この問題においては特に考えなくて大丈夫です

### 状態
この問題においては特に考えなくて大丈夫です
腕を引くことで状態は変化しません

### 報酬
スロットマシンを引いたことで得られる利益です
**今回は，報酬がガウス分布（正規分布です）で表されるとしています**
**よって，報酬の真値を$Q*(a)$とした場合，実際に得られる報酬は分散1，平均$Q^{*}(a)$に従うものになりますね**

図のイメージ
この場合はスロットが5本の腕を持つとしています

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/19a93283-8332-2718-f9bf-c985651ac8ac.png)


### 価値
この問題においては推定される報酬になります
推定される報酬とは・・・
普通に考えたら何回か試してみてその平均をとりますよね
それで一番確率が高そうなやつを選ぶと思います
それです
それを**標本化平均法**といいます
数学チックに書くと
t番目のプレイで，ある行動aをk回とったときの価値の推定（この腕をひくことの価値）を

![aaaaaaa](https://user-images.githubusercontent.com/37980935/42994457-1b2355a2-8c49-11e8-89ab-63c9c44afa21.JPG)

と表します
t回プレーして（スロットの腕を引いた合計数）で，そのうちあるスロットを引いたのがk回だったら，k個分データがあるので，平均をとれば推定できそうです．
しかも行動する回数が増えれば増えるほど，真値に収束します（大数の法則ですね）

ちなみにプログラムに実装するときはメモリの使用が多くなってしまうので
r(k+1)をその時に得た報酬とすると

![aa](https://user-images.githubusercontent.com/37980935/42994467-205e6e9e-8c49-11e8-8f9c-24513b01ad68.JPG)

で更新しましょう！
ちなみにこれはそんなに難しい式変換ではなく，要は更新分が平均にどう加算されるかをみているだけです

## 難しい点
さて，さっき平均をとって推定するといいました
あれ？何回試すの？本気でプレーするタイミングはいつになるの？
という疑問が湧いてきます

つまり

何回データをとって，何回本気でプレーするんですかというわけです．
さらに詳しくいいます
このゲーム，この腕きっと良いっていって引くんですけど

- ずっとこれがいいって思った腕を本気で引き続けて，確率が正しい値に収束しても，実は他のが高い場合がある
- かといってずっとあちこち浮気して引きまくってデータを集めてたら終わりが見えない

というわけです

ここで出てくるキーワードが**貪欲法と，知識利用と探索**になります

まず貪欲法です
これは，その名の通り，これだって思う腕を本気で引きまくる方法です
つまり，推定される価値が最も大きい腕を引き続けます
これは自分が知っている知識を利用し続けるので，**知識利用**になりますね！

しかし，これだけでは進化しません
なので，あるタイミングでデータを集めにいきます
これが**探索**になります！

この探索をランダムでやるのが，ε貪欲法になります！
大体10%ぐらいでデータ取りに行きます．その時の気分ですね

ただ，この手法，見た目普通なのに，かなり強力で，今でも使用されることもあります
人間っぽい学習方法だからかもしれませんね

## 再び問題設定
ではなんとなくできそうなので具体的にこの問題を解きます
ここで注意なのはスロットマシンも確率的なので，2000個のスロットマシンの真値は，平均0分散1のガウス分布から， 行動aの報酬の真値Q(a) を生成します．
なので！

- **行動選択によって得られる報酬も確率分布を利用**
- **そもそもの真値も確率分布を利用**

になっていることにご注意ください

まず，確率を考えるので，このスロットマシンを2000個用意します笑
普通のパチンコ屋って何台あるのでしょうか笑
（教科書の問題設定なので許してください）
そして，それぞれのスロットの行動aに対する報酬をQ(a)とします

なので例えば，ある腕適当にを引くってなったら2000個おらって引きます
それで行動を推定していくというわけですね！

**ここで注意点は，そのスロットマシンそれぞれで，引く腕は異なります**
なのでここでいう行動aは，配列的になっています
マシン1では，腕1を引いて，マシン2では，腕4をひいて．．．．ってなります
しかもそれぞれスロットマシンで貪欲か探索かを考えて
吟味するので，ここは勘違いしないようにしてください！！


![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/c7d2c562-11eb-8e0b-c377-a3e9afe87494.png)

# プログラム実行について（Usage）
上記の問題を解法するためのプログラムですが、

```
$ python n_bandit.py
```

で回ります

# 結果
![Figure_2.png](https://qiita-image-store.s3.amazonaws.com/0/261584/9840f2ee-ff3b-cedc-cc1a-44509b3a4754.png)

![Figure_3.png](https://qiita-image-store.s3.amazonaws.com/0/261584/c27a4062-b0ae-5e0d-eb09-16724a3628ad.png)

# 考察
- 基本的にランダムにした方が、探索を続けるので貪欲法より報酬が多くなる
- 試行回数が少ないときはεが大きい方が良い
- 試行回数が多くなると、εが小さい方が良い