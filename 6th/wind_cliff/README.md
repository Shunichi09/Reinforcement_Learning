# 6th/wind_cliff
Q学習とSarsaの比較になります

# Outline
- Sarsa学習について
- Q学習について
- 風のある地を歩く（Sarsa学習）
- 崖を歩く（Sarsa学習 + Q学習）

# Sarsa学習について
前回ではTD学習として下記の学習法を説明しました．
![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/f8fe9172-fd0a-eba6-c9c2-a916773986ec.png)

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/a4ee495c-2ad4-923c-de3b-5588bad64c66.png)

ここで価値関数で更新を行うのではなく，
行動価値関数で更新を行うとします．
これは本質的には何もかわりません

最適方策を見つけるためにモンテカルロ法で行ったときと同じです（正確に言うと，モデルがない場合は価値関数では実装できません，もちろん価値関数を求めるだけの場合は別ですが）

すると，，，

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/5aaa1fe4-e4d6-eab0-dbb1-12ae28acd365.png)

となります
ここでのQはε-greedyにすることが多いそうです
εでランダムの動きが起きるので．いろいろ経験できます
しかしここでポイントなのは

**差分をとっているQ(推定方策)もε-greedyなんです**

なので最終的にこのままではε-greedyでの行動価値関数になるわけです．
なのでεの値を時間に応じて小さくしていく
例えば，1/tにするといった対策をとることで，最終的には最適方策になります

ちなみにSarsaというのは，Q(s, a), r, Q(s, a)
の中身の頭文字をとって，Sarsaというわけです！

# 風のある地を歩く問題
では実装してみましょう
教科書の問題を参考にするとこんな問題です

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/1aff1b15-0d61-6e5a-a4e8-4df2cfe78d3d.png)

start(3, 0)からgoal(3, 7)まで行くってことですね
行動は【”up”，”down”，”left”，"right"】
でも風があるので，．．．どうなるかというわけです

# 風のある地を歩く問題（Usage）

```
$ python wind.py
```


# 風のある地を歩く問題（結果）

## 学習の様子
（最初はなかなかゴールに辿りつけてませんがどんどんたどりつけるようになります）
![Figure_1.png](https://qiita-image-store.s3.amazonaws.com/0/261584/38ca8cc6-7240-3658-e0dc-4d916a0629fc.png)

## ε-greedy方策での行動価値関数での最適経路

![Figure_2.png](https://qiita-image-store.s3.amazonaws.com/0/261584/e605c6b7-9832-3cf2-2076-4e7d07356570.png)


学習できましたね！教科書と同じ結果です
直接的にはたどり着けないのでこうやって大回りして逆からアプローチしているわけです
ここでの考察としては，この問題はいつ終了するかわからないのでモンテカルロ法では学習が難しいです
でもTDであればこのように学習することができます

ちょっとだけ補足します
教科書では0.1にアルファが設定されていますが，それだと教科書より学習に時間がかかってしまいます
なので0.5に設定してください（原著のgithubはそうなってました）
あと，numpy等の実装を行う際に，argmaxは．一番初めの最大のインデックスを返してしまうので，最初の方の学習がうまく進みません
ここで，argmaxのインデックスが複数存在する場合は，その中からランダムに選ぶようにしましょう
コードは最後にのせておきます

# Q学習について
さて，次にいよいよ登場しましたQ学習について説明していきます
Q学習は次の式で更新されます

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/7ba6328a-d570-5160-d972-ade0e6769a75.png)



ここで何が違うのかを考える必要があります

Q学習は更新するときに次の状態の最適方策で更新します
つまり！！推定している方策と，挙動している方策が異なります
なので最後は最適に落ちるわけです

ちなみに，推定方策と挙動方策の違いの，この話は方策OFF型でももでてきましたね！
なので，これはTDの方策OFF型というわけです
ちなみにお分かりのようにSarsaは方策ON型です


# 崖の近くを歩く問題

この違いが良く出る問題が教科書にのっていたのでやってみましょう

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/69a7aa58-b011-75a0-967f-d4f232368944.png)

こういう問題です
最適経路は示したようになるのが正解ですね

# 崖の近くを歩く問題（Usage）

```
$ python cliff.py
```

# 崖の近くを歩く問題（結果）

最適経路

![Figure_2.png](https://qiita-image-store.s3.amazonaws.com/0/261584/199525e2-2a6d-e2be-265c-00881a8116d0.png)

学習結果
（教科書の例は平滑化（何回かおなじ回数やって平均とってるっぽい）していますのでこの結果はふーん程度で大丈夫かと思います）
なんとなくQ学習のが報酬はすくなそう

![Figure_1-1.png](https://qiita-image-store.s3.amazonaws.com/0/261584/58d6b574-55d9-ecc6-25ad-077a9ff9c7eb.png)


結果をみるとSarsaは安全な道を選んでいるのに対して．
Qは，ぎりぎりを攻めています
この違いはなんでしょうか？

これはさっき言った，ε-greedyです
つまり，ε-greedyでの行動価値関数では，
崖付近でランダムに動く可能性が残っている（正確に言うと崖付近は価値が低くなる）ので，遠回りするんです

一方，Q学習については別です
最適方策を一生懸命学習します
しかし，学習しているときの報酬はSarsaの方がよいです
次がランダムに行動するのをふまえているからです
Qは自分が最適な推定方策なのに、挙動はε-greedyなので、崖の近くでランダムに動かれると落ちるわけですね

使い分けが難しそう

本当はここまでで終わるつもりでしたが
せっかくなら、DQNとかもやってみようと思います