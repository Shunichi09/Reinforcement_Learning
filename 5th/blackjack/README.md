# 5th
モンテカルロ法の概要とそのプログラムの説明です。

# Outline
1. 前回とのつながりから～モデルがない場合は？
2. モンテカルロ法
    3. ブラックジャックのプログラム
    4. ブラックジャック攻略

# 前回とのつながりから～モデルがない場合は？
前回，第4回の動的計画法では，モデルが分かっている状態では話が進んでいました．

モデルがわかっているというのは，自分が行った行動に対して結果がどう出るか
つまり状態遷移確率が分かっていたということですね
右に動いたら右にいくのもそうです

ですが，分からないときもありますよね
そもそどんなゲームか分からない場合もそうです
行動を選択してどういう状態に移るのか
どんなゲームかわからないと出来ません

これをモデルが存在しないといいます

ただし！
今回はそういうものの中で、どういうゲームかは分かっているけど（モデルがあるけど），動的計画法だと厳しいものについて考えてみましょう

例えば！対戦型のカードゲーム、ブラックジャックを想定してみましょう

ルールはリンクをご覧ください

https://ja.wikipedia.org/wiki/%E3%83%96%E3%83%A9%E3%83%83%E3%82%AF%E3%82%B8%E3%83%A3%E3%83%83%E3%82%AF

トランプを引き合うゲームなので
場がどう遷移するかは分かります
モデルはあるといえますが、、、

しかし、動的計画法は、遷移確率と、報酬の期待値を必要とします

プレイヤーがある状態でストップをかけた場合の報酬の計算、つまり実際に勝てるかどうかを計算するのは難しいです

例えば前回のギャンブラーの問題では、報酬ってすぐわかりますよね
100に行ってなかったらもうそれで0です

でも、今回はストップと言ってから、実際に勝ってるのかそれとも負けてるのかわかりません
(モデルがあるので計算することは可能です、しかし複雑です)
確率が分かっていればいいんでしょうけど

今回はそういう時にどうすべきか考えます

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/c5e818bf-dd3f-c369-a4c3-65d58cd13e5f.png)


# モンテカルロ法の概要
場がどう動く、期待報酬が簡単には分からないときに使えるのがモンテカルロ法です
モンテカルロシミュレーションという言葉を聞いたことがあるかもしれませんが，多分それと同じです．

**まんべんなく**数うって平均とれば分かるでしょ戦法です

つまり、ある方策に則り、たくさんのケースを実際に体験して、そこからある方策のもとでの状態価値を求められませんかね？という作戦です

**ここで、ある方策のもとでの状態価値の推定を方策評価というのでした**

状態価値が分かれば方策改善ができそうです
前回の動的計画法とおなじ流れですね

アルゴリズム自体はさほど難しくありません
イメージした通りだと思います
以下流れです

1. ある方策πの元でエピソードを作る
2. エピソード上で体験した状態を保存しておく
3. エピソードが終わり報酬等が確定した時点で，逆から計算して，状態の価値求める
4. 1-3を繰り返し行い，各状態で平均をとる

です
1エピソードとは，1ゲームが終了することを意味します（ブラックジャックでいえば，勝ちか負けか確定するタイミング）
こうすれば，各状態でどれくらいの価値だったのかがわかります
以下の図がイメージ
ブラックジャックを想定しています

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/e908f764-a603-5674-b9f2-74fd368982df.png)


逆から計算するのは，なんとなくわかると思います
最終のものが確定しないとわからないので

ではさっそくやってみましょう
ブラックジャックを想定します

# ブラックジャックの問題（方策評価）
ブラックジャックのルールは以下のものでした
（http://yamaimo.hatenablog.jp/entry/2015/10/01/200000）

> **ルールの概要**
トランプを使用する。
トランプは無限デッキあると仮定する。（＝カードの出る確率は変化しない）
Aは1もしくは11として扱う。
2〜10は数字通り扱う。
J, Q, Kは10として扱う。
カードの合計が21を越えず、出来るだけ21に近い方が勝ち。（同じなら引き分け）
**プレイの流れ**
ユーザーにカードが2枚オープンで配られる。
ディーラーにカードが1枚はオープン、もう1枚はクローズで配られる。
プレイヤーは以下の行動が出来る。
ヒット（カードをもう1枚引く）
スタンド（カードを引くのを止める）
カードの合計が21を越えたら、その時点でプレイヤーの負け。
スタンドするか21を越えるまでは、何度でもヒット出来る。
プレイヤーがスタンドを選択したら、ディーラーは伏せていたカードをオープンにし、カードの合計が17以上になるまでカードを引く。
カードの合計が21を越えたら、その時点でプレイヤーの勝ち。
ディーラーのカードの合計が21以下の場合、カードの合計を比べる。
カードの合計が21に近い方の勝ち。
同じなら引き分け。


さらに報酬は

- 勝ち1
- 負け-1
- 引き分け0

ここでいう状態は

- 相手が見せているカード(1-11)
- 自分の和(12-21)
- Aceを11としてつかっているか否か

の200個です
ちなみになんで自分の和12-21なんだよと思う方いると思いますが
12以上にならないとストップ（勝負）する意味ないんです

自分が11（Aceを持っていたら）だったらどうしますか？
どんだけ大きいカード引いても10です（キングとか絵札は10なので）
なので普通引きます
だから12-21の状態だけでいいんです

**また，Aceの取扱についてですが，基本的には11で考えます**
で21を超えるようであれば1として考えなおします
なぜかというと，1で考えておくことのメリットがないからです
例えば，自分が（4, Ace）を持っている時，15として考えるべきです
もし1として考えると5になって，引かないといけなくなります（12-21ですから）
他のケースについてもすべてそうです
なので最初は11と考えて，21を超えてはじめて，1として考えます

ここで
評価したい方策を
- **20以上でstop（勝負に出る）とします**

ではさっきのイメージとアルゴリズムに乗っ取って，方策を評価してみましょう
200個の状態をすべてランダムではじめます
それから遊んでみて，報酬がでたら後はそれを蓄積して，平均とるだけです

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/4a0f8274-461d-7825-9874-4baeae5a8e8d.png)

# ブラックジャックの問題（方策評価）（Usage）

```
$ python blackjack1.py
```

# ブラックジャックの問題（方策評価）（結果）

結果です（公式の結果と比較検証は行いましたたぶんあっています）
3Dplotについては[こちら](https://qiita.com/MENDY/items/e5e14520201f82ef8a3c)をご覧ください

上が50万回プレーして，Aceを11として使っているとき
![Figure_1.png](https://qiita-image-store.s3.amazonaws.com/0/261584/9d68036b-9f00-04e3-c537-f2dd33c05568.png)

下が50万回プレーしてAceを11として使ってないとき

![Figure_1-2.png](https://qiita-image-store.s3.amazonaws.com/0/261584/64ecf3ca-b8a7-2767-1553-0c33e512ed58.png)

更に1万回の場合です。
Ace あり
![Figure_11.png](https://qiita-image-store.s3.amazonaws.com/0/261584/83503f57-013a-d03c-b773-ef866bcab8b1.png)

Aceなし
![Figure_21.png](https://qiita-image-store.s3.amazonaws.com/0/261584/baea5491-51eb-7a5c-9e9a-cc8ffdca2655.png)


プログラム的にはdealerクラスとplayerクラスを作りそれで，実際にやってみてるだけです
難しいのは報酬のところかもしれません
そもそもこのゲーム，プレイヤーがくず手になった時点でプレイヤーの負け確定です
なので

としています
本当は，先に確定してreturnすべきなのですが，今回は，dealerもプレーしてから考えてます

ちなみにこれ1万回ぐらいでやると，もっとぼこぼこします
なぜかというと，体験した，エピソードが少ないからです（各状態で保存してあるものが少ない）

さて，感想は圧倒的に不利ってことです笑
自分が-にいる状態が多いですね笑

これで，ある方策での方策評価を行うことができました

では，次にこの方策をどう改善していけばいいのでしょうか？
これを改善できれば，ブラックジャック攻略できます

# ブラックジャックの問題（方策改善と方策評価）

動的計画法では，

<img src = "https://latex.codecogs.com/gif.latex?\begin{align}&space;Q^\pi(s)&space;&=&space;E_\pi&space;\bigl[R_t&space;|&space;s_t&space;=&space;s,&space;a_t&space;=&space;a]&space;&=&space;E_\pi&space;\bigl[\sum_{k=0}^{\infty}&space;\gamma&space;r_{t&plus;k&plus;1}|s_t&space;=&space;s,&space;a_t&space;=&space;a]&space;&=&space;\sum_{s'}&space;\rho_{ss'}^a&space;\bigl[R_{ss'}^a&space;&plus;&space;\gamma&space;V^\pi&space;(s')&space;]&space;\end{align}" />

のように方策改善しましたが．．．
ここでいう状態遷移と報酬はわかりませんっていう話がさっきでてました

**ただし，今回は行動価値がわかります．ようはどんな行動（ブラックジャックだと引く，ストップ）でもらえる収益（報酬の和）がわかりますよね**

なので，単純に，一番いい行動価値になる行動を各状態でとればいいんです

<img src = "https://latex.codecogs.com/gif.latex?\pi&space;=&space;arg\max_aQ(s,&space;a)" />

？
ってなりそうですが，モンテカルロ法では，実際に体験している，状態と行動を対で保存することができます
なのでこの手法をとることができます
（動的計画法はどちらかというとエピソードを体験するというよりは，もともととける式の近似解法を行っているイメージです）
**これのいいところは環境のモデルがいらないところです**
状態遷移は考えてませんね　とりあえずもらえる収益が高い行動を選ぶだけです

ちなみ行動価値は

>さきほどはある状態$s$にいて，行動$a$をとり，そのあとは，方策$\pi$に従った場合の行動価値関数を定式化します

><img src = "https://latex.codecogs.com/gif.latex?Q^\pi(s)&space;=&space;E_\pi&space;\bigl[R_t&space;|&space;s_t&space;=&space;s,&space;a_t&space;=&space;a]&space;=&space;E_\pi&space;\bigl[\sum_{k=0}^{\infty}&space;\gamma&space;r_{t&plus;k&plus;1}|s_t&space;=&space;s,&space;a_t&space;=&space;a]">

>a_t = aという行動の条件が加わっているだけですね
>つまり，状態価値関数の中で，ある時刻$t$の行動が指定されているイメージになります

>上記2つをまとめるとこういう感じ
>まとめているというか書いてるだけですが．．．

>![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/5a956af1-6e1d-8b58-5949-17ef3189892e.png)

ちなみに，これも，greedyに改善しているので，方策改善定理が使えます
一番いいやつ選んでるからなんとなく改善されそうです

では，さっきのアルゴリズムの流れを少し変えてこの話を書いてみます

方策πを初期化

1. 方策$\pi$の元でエピソードを作る
2. エピソード上で体験した状態と行動のペア(s,a)を保存しておく
3. エピソードが終わり報酬等が確定した時点で，逆から計算して，行動価値Q(s,a)求める
4. 方策π(s)を行動価値Q(s,a)が大きくなるように改善する
4. 1-4を繰り返し行い，各状態で平均をとる

はい
変わったのは行動対の報酬を保存しているだけです

この手法をモンテカルロES法といいます

また前提として
- エピソードの開始をランダムな状態から始めること（すべての状態を観測したいので）
- 方策評価に無限のエピソードを行うことを仮定していることがあります

実際，上のアルゴリズムでは，方策評価のエピソードを無限に行わず（この考えは動的計画法と同じです），1エピソード毎に方策改善をしています
動的計画法では，証明されていましたが，このモンテカルロでは，形式的にはこのアルゴリズムでの最適方策への収束は証明されていないそうです

長くなりましたが，ここからこのアルゴリズムを用いて，ブラックジャック攻略法を見つけていきましょう！！！

![thumbnail_card_all.jpg](https://qiita-image-store.s3.amazonaws.com/0/261584/a291c5d1-24e4-bd5e-eb5f-cd623a054cef.jpeg)

とはいってもやることはあんまり変わりません
状態行動対を保存していくように書き換えるだけです

ただ注意点！
先程も言ったように、モンテカルロ法は、開始点探査を前提としています
最適方策で選び続けると、固定されたものしか選ばれないので、、、

なので、状態行動対はランダムにスタートさせます

さらに初期方策は、先程の20以上だとストップを採用します

またまた繰り返しになりますが、この初期方策とは、ランダムに選ばれた状態行動対の後に従うものなので！

流れとしては、

1. ランダムに状態行動対を作成
2. エピソードを始める
3. 最初の行動だけは、1で決めたものを使う
5. 次から方策に従う
6. 方策改善をする

これらを何百万回も繰り返す
です

# ブラックジャックの問題（Usage）

```
$ python3.6 blackjack2.py
```

# ブラックジャックの問題（結果）

結果はご覧の通り！

黄色がHIT！引く
緑色がStop!止まって勝負です！

これらは，100万回のエピソード後です

Aceを11として利用する場合


![aaaaa.png](https://qiita-image-store.s3.amazonaws.com/0/261584/d43452b7-99df-778c-3338-a929d7b37c58.png)


Aceを11として利用しない場合

![bbbbbb.png](https://qiita-image-store.s3.amazonaws.com/0/261584/4c1e4834-a584-2936-66ae-1cb6b0ca3047.png)



まぁほぼ教科書と一緒です
（ちょっとplotの仕方がいまいち（ほんとは棒グラフ的なやつでやった方がいいと思います）なので，境目のところがうまくいってませんが）

ちゃんと中身をみると行けてそうですね！！

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/40769c1a-002d-f826-c699-c1bbb54af05e.png)

これによって！
ブラックジャックをやるときの戦術が学習できましたので，今度カジノに行く機会があったらやってみます

日本にもカジノができることですし，強化学習を用いてさまざまなゲームの攻略をどうにかできませんかね．．．

以下プログラムです
ちょっとごり押しで書いたのであんまりきれいじゃないです
まぁ今までのもきれいじゃないのであれですが，練習します！

第五回はここまでにします！
ただ，まだ補足があるので次回5-2回で説明します．