# 4th
このフォルダでは、動的計画法について説明します

# Outline
1. 前回とのつながりから～方策評価
2. 方策評価と方策改善
3. 迷路の問題
4. ギャンブラーの問題

# 前回とのつながりから～方策評価
前回のまとめとして
**状態価値関数と行動価値関数を定式化し，最適状態価値関数，最適行動価値関数を定義しました**

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/7079f892-da2e-2de9-53ac-80d8e54ff347.png)


さて，強化学習において大切なのはその状態価値関数をしること（行動価値関数の場合もありますが）です
そうすれば，今自分のもっている方策πを評価できます．
そして改善へとつなげることができるのですが．．．

さて，ここからこんな風に考えます
ある冒険を考えてください．
前回と同様に，今いる場所から，次へ動くときにランダム（すべて1/4で動くとします）
これで自分の行動の指針である方策πが定義できました．


![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/23363f3d-f41b-aa7c-2647-4305a82bb8c7.png)


では，次に気になるのがこの方策πがどれくらい正しいのかってことです
冒険でランダムに動くということは損でしょうたぶん（ラスボスの位置は決まってますので）

なので，この方策πがどれくらい正しいかを知るために，状態価値関数を方策πのもとで求めていきます．

冒険でいえば，今街にいるとして，そこからランダムに場所を選ぶとした際の街にいることの価値を求めます．

結論からいうとこの状態価値関数は以下の式で求められることになります．
今回はもはやこの式がめちゃくちゃ重要なのでそれ以外は気にしなくてもいいぐらいです

<img src=https://latex.codecogs.com/gif.latex?\begin{align}V_{k&plus;1}(s)&space;&&space;=&space;E_\pi&space;\bigr[&space;r_{t&plus;1}&plus;&space;\gamma&space;V_k&space;(s_t&space;&plus;1)&space;|&space;s_t&space;&plus;&space;s]&space;\\&&space;=&space;\sum_a&space;\pi(s,&space;a)&space;\sum_{s'}&space;\rho_{ss'}^a&space;[R_{ss'}^a&space;&plus;&space;\gamma&space;V_k(s')&space;]\end{align}>

いやいやどういうこと？ってなります
まず，完全にモデルが分かってるとすると，一番最初の式は状態$S$に関する式なのだから，状態の数だけ式ができて，それを解けばよいってことになりますね．
ただ，状態がたくさんある場合はやりづらくて仕方がないし，もし少ないとしても，式をいちいち立てるのはめんどくさいです．

そこで！！

さっきの近似解法を用います．
kというのは計算回数です
つまり，一回計算したときの値を使って次々に更新していくことで，最終的にある方策$\pi$のもとでの状態価値関数を取得できるというわけです．

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/d054378e-96ce-9b42-d62a-90031e1ceebb.png)

図でいうと

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/3fbc99b1-0ecb-d8e3-1c07-5cf336e87962.png)


ということになります．

つまり作業としては，何回も何回もある方策πにのっとって計算しまくり，各状態の**暫定の状態価値関数**を求めていきます．そうすれば，自然とすべての値が更新されていき．．．いつのまにか，ある方策$\pi$での状態価値が求まることになります！
証明は一応できるみたいです

## 前提条件
さて，順番が入れ替わってしまいましたが，このやり方（これこそが動的計画法なのですが）で解けるのは，**環境のモデルが完全に分かっているときのみ**です

なので，バックアップ（更新のために溜めておく）のは，1つ前の各状態になりますね
すべての状態を溜めておく必要があるので，完全バックアップと言われます

次回ででてくるモンテカルロはまた違うものをバックアップします

# 方策評価と方策改善
## 方策評価
さて，さきほどの式である状態における状態価値を計算できることがわかりました．
ここで大切なのは，方策πのもとで！ということです．

この，ある方策πのもとで，状態価値を計算することを**方策評価**といいます

アルゴリズムとしてはこんな感じ

1. すべての状態価値V(s)を0とする
2. 1回前の計算結果V(s)を用いて，更新
3. 更新量が小さいかチェック
    4. ちいさければ終了
    5. 大きければ，2に戻る

更新式は，次の通りです

<img src=https://latex.codecogs.com/gif.latex?\begin{align}V_{k&plus;1}(s)&space;=&space;\sum_a&space;\pi(s,&space;a)&space;\sum_{s'}&space;\rho_{ss'}^a&space;[R_{ss'}^a&space;&plus;&space;\gamma&space;V_k(s')&space;]\end{align}>

# 迷路の問題
問題はこんな感じ（[教科書](http://www.morikita.co.jp/books/book/1990)より引用）

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/26434fe4-d42e-1c57-aefc-e25d48422292.png)

つまり，上下左右に動ける人がいて，ランダムに動いた場合の方策評価をしてください
ゴールはの状態価値は0.0です
という問題です

モデルが分かっているので，今回はすべての状態価値を1つ前の状態価値で計算します

なのでアルゴリズムとしては，先ほども書いたように

1. 初期化
2. すべての状態で，1つ前の状態を更新
3. 繰り返し

更新に必要な変数はすべて求まっています

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/a1da1a6a-6086-886e-a7c4-42e2b72a8bee.png)


あとはこれをプログラムに書きおこすだけです
ちなみに，どの状態に移り変わっても報酬は-1なので（状態遷移にマイナスの報酬が入る設定です）
また，移動できるのは右左上下
よくプログラミング大会とかの考えと同じで地図は，行列で管理しましょう．

# 迷路の問題（Usage）

```
$ python meiro.py
```

# 迷路の問題（結果）
結果だけ

```
value function is (iteration_num = 0)  = 
[[ 0. -1. -1. -1.]
 [-1. -1. -1. -1.]
 [-1. -1. -1. -1.]
 [-1. -1. -1.  0.]]
value function is (iteration_num = 1)  = 
[[ 0.   -1.75 -2.   -2.  ]
 [-1.75 -2.   -2.   -2.  ]
 [-2.   -2.   -2.   -1.75]
 [-2.   -2.   -1.75  0.  ]]
value function is (iteration_num = 1999)  = 
[[  0. -14. -20. -22.]
 [-14. -18. -20. -20.]
 [-20. -20. -18. -14.]
 [-22. -20. -14.   0.]]
```

こんな感じになります（教科書と同じです）
ゴールの状態は0になっており，**ゴールから遠いところの価値が小さくなってますね**
完璧です

## 方策改善
さて，この答えは，**ランダム方策（ランダムに上下右左を選ぶ）における状態価値でした．**

強化学習における目的は

**価値を知って，その価値を最大化する方策を得ることでした**

つまりこれでだけでは，ある方策の良さしか評価できないので困ってしまうわけですね

ここで**方策改善**という考えが出てきます

このランダム方策がどれだけ良いのかをしってその方策を改善したいなというわけ

ではそれをどうやって知るのか
それは行動価値関数をもちいてしることができます

行動価値関数は以下の式で表されます

<img src=https://latex.codecogs.com/gif.latex?\begin{align}&space;Q^\pi(s)&space;&=&space;E_\pi&space;\bigl[R_t&space;|&space;s_t&space;=&space;s,&space;a_t&space;=&space;a]&space;&=&space;E_\pi&space;\bigl[\sum_{k=0}^{\infty}&space;\gamma&space;r_{t&plus;k&plus;1}|s_t&space;=&space;s,&space;a_t&space;=&space;a]&space;&=&space;\sum_{s'}&space;\rho_{ss'}^a&space;\bigl[R_{ss'}^a&space;&plus;&space;\gamma&space;V^\pi&space;(s')&space;]&space;\end{align}>

例えば，今ある状態sにおいて行動aをとり，その後方策πに従った場合の状態価値が，状態sからずっと方策πにしたがった場合よりも大きければ．．．
行動を変えた方がいいことになります

図で表すとこんな感じ

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/a9a55751-921d-fc50-c018-d6906436de7f.png)


この考え方？というかこれが成り立つことを方策改善定理といいます．
証明は本等にお預けします！が証明できます

そして，ここでgreedyという考えがでてきます

つまり，ある状態sにおいて，その時にとれる最も高い報酬をもらえそうな行動を選択すれば，そのあとは最初から方策πによって行動するよりはいいだろってわけです．（すごく当たり前のことをいっています，方策πは確率ですからね，そりゃ最初の行動をもっともいいやつに確定させておけば高くなりますよね）

ちなみにこの考えは確率にも適用できます
つまり，その時にとれる最も高い報酬をもらえそうな行動に高い確率を振っておくということでも，この方策改善定理は成り立ちます
期待値が大きくなるので，なんとなくわかるかと思いますが．．．

よってこの方策改善定理に乗っ取れば方策を改善できるので，すべての状態において方策を改善すればよいわけです（上下右左，どれを選ぶか決めてしまう）
つまり

1. ある方策に乗っ取って状態価値関数を計算
2. 収束するまで計算
3. 収束したら，方策を改善（greedyに選ぶ）
4. 1に戻る

という方策改善と方策評価を繰り返し行っていくことによって，（方策反復といいます）
**最適方策における最適な状態価値求めることができるのです**

繰り返しの打ち切りは方策改善が終わるまでやります（変化しなくなるまで）

まだ続きます（長くなりますが．．．）

## 価値反復
さて，上記のやり方だと，上手くいきそうですが，いちいち方策評価をしなければならないという欠点があります
**そんな計算待ってられないよ！！！怒**
というわけです

そこで

収束性が証明されている手法に切り替えます（考え方は変わりません）

アルゴリズムはこうです

1. 方策評価を1ステップのみ行う（1回分）
2. 方策を改善する（一番いいやつ選ぶようにする）
3. 1に戻る

という作戦です．これを式で表すと

<img src=https://latex.codecogs.com/gif.latex?V_{k&plus;1}(s)&space;=&space;\max_{a}&space;\sum_{s'}&space;\rho_{ss'}^a[r_{ss'}^a&space;&plus;&space;\gamma&space;V_k(s')]>

となります．
**どっかでみたことある！**ってなります
そうですこれ，ベルマン最適方程式を更新用に書き換えただけです

この式が表すのは，
**最適方策（この場合はgreedy方策）のもとでの状態価値を求めていることになります**
1回分しか計算しないのでこのように漸化式的に表せます！！

図でやるとこんな感じ（方策改善のところだけですが笑）

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/f16c7e00-71d2-8869-a538-543a6260546c.png)


しかもこれ，最適方策の下で，最適状態価値に収束することが証明されています．

今回は以上なのですが，プログラミングしたら面白そうな問題がのってたのでやります

## ギャンブラーの問題（[教科書](http://www.morikita.co.jp/books/book/1990)より引用しています）
問題としてはこうです
コインを投げるギャンブラーがいます
この人は，コインを投げて，表がでれば，自分が表にかけた金をすべてもらえますが，裏がでればすべてを失います
ゲームが終了するには

- 賭け金がなくなる（所持金が0になる）
- 目標の100ドルをもらって，カジノを退出

であり

ギャンブラーは1回のコイン投げでいくらをかけるのかきめなければならないという問題です

掛け金は整数とするので，

自分の状態はギャンブラーの所持金（1,2,3, ... 99）
行動はいくら賭けるか(1, 2, ... min(s, 100-s))　どっちか小さいほうなので（100ドルもらえばいいので，100ドルを結果的に超すような賭け金は設定しません）

コインの表が出る確率は0.4です

![image.png](https://qiita-image-store.s3.amazonaws.com/0/261584/a2fa0a73-9913-7b02-e244-26fa484a9bef.png)



やってみましょう！！

価値反復を使います
さっきの迷路と同じように状態価値を全部0で初期化して．．．

選択するときはgreedyにします
**すこしだけ注意というか考え方なのですが，今までは，前の状態価値$V_s$を用いて的な話をしてました．ただ，今回のプログラムは更新したら同じサイクルの中でもその更新した状態価値$V_s$を使ってます**

これも，まとめて更新するのと，結局同じことをしてることになるはずなので（たぶん），まとめて計算しても最後は同じ値になるはずです．

# ギャンブラーの問題（Usage）

```
$ python gamble.py
```

# ギャンブラーの問題（結果）

![Figure_1.png](https://qiita-image-store.s3.amazonaws.com/0/261584/bd91606f-fba2-ca1b-3902-1529f96ef8d0.png)

上が最適の状態価値
下が最適方策です

特徴的な形になりましたね
最適方策がすごく変な形をしてます
50において，すべての金をかけるのが良いのは，それがもっとも確率が高くなるからです．（50もってて，50もらえる可能性があるので）この問題は，かければかけるほどじり貧になります（0.4という確率から）

なので，勝負に出れるときは勝負に出た方がいいんです
50の時は特に顕著にそれが出ている感じです

なお最適方策を求めるときに，このように，マージンを持たせているのは，計算誤差があるからです．

こうしないときれいに形でないかもです
（確か公式のプログラムもうまくいかないはず）